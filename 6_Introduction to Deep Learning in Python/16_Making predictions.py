#Making predictions
#The trained network from your previous coding exercise is now stored as model. New data to make predictions is stored in a NumPy array as pred_data. Use model to make predictions on your new data.
#
#In this exercise, your predictions will be probabilities, which is the most common way for data scientists to communicate their predictions to colleagues.

# Specify, compile, and fit the model
model = Sequential()
model.add(Dense(32, activation='relu', input_shape = (n_cols,)))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='sgd', 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])
model.fit(predictors, target)

# Calculate predictions: predictions
predictions = model.predict(pred_data)

# Calculate predicted probability of survival: predicted_prob_true
predicted_prob_true = predictions[:,1]

# print predicted_prob_true
print(predicted_prob_true)

#Answer
Epoch 1/10

 32/800 [>.............................] - ETA: 0s - loss: 5.3679 - acc: 0.3438
768/800 [===========================>..] - ETA: 0s - loss: 2.3374 - acc: 0.5872
800/800 [==============================] - 0s - loss: 2.3207 - acc: 0.5825     
Epoch 2/10

 32/800 [>.............................] - ETA: 0s - loss: 2.9760 - acc: 0.3438
768/800 [===========================>..] - ETA: 0s - loss: 1.2286 - acc: 0.6055
800/800 [==============================] - 0s - loss: 1.2019 - acc: 0.6088     
Epoch 3/10

 32/800 [>.............................] - ETA: 0s - loss: 0.6071 - acc: 0.7500
800/800 [==============================] - 0s - loss: 0.7325 - acc: 0.6650     
Epoch 4/10

 32/800 [>.............................] - ETA: 0s - loss: 0.5543 - acc: 0.6875
800/800 [==============================] - 0s - loss: 0.7487 - acc: 0.6600     
Epoch 5/10

 32/800 [>.............................] - ETA: 0s - loss: 0.5900 - acc: 0.6875
672/800 [========================>.....] - ETA: 0s - loss: 0.6145 - acc: 0.6890
800/800 [==============================] - 0s - loss: 0.6201 - acc: 0.6800     
Epoch 6/10

 32/800 [>.............................] - ETA: 0s - loss: 0.6343 - acc: 0.6250
640/800 [=======================>......] - ETA: 0s - loss: 0.6559 - acc: 0.6766
800/800 [==============================] - 0s - loss: 0.6427 - acc: 0.6887     
Epoch 7/10

 32/800 [>.............................] - ETA: 0s - loss: 0.5614 - acc: 0.6562
384/800 [=============>................] - ETA: 0s - loss: 0.6341 - acc: 0.6667
800/800 [==============================] - 0s - loss: 0.6589 - acc: 0.6863     
Epoch 8/10

 32/800 [>.............................] - ETA: 0s - loss: 0.6180 - acc: 0.6250
576/800 [====================>.........] - ETA: 0s - loss: 0.6594 - acc: 0.6649
800/800 [==============================] - 0s - loss: 0.6555 - acc: 0.6700     
Epoch 9/10

 32/800 [>.............................] - ETA: 0s - loss: 0.6671 - acc: 0.6250
384/800 [=============>................] - ETA: 0s - loss: 0.6359 - acc: 0.6562
736/800 [==========================>...] - ETA: 0s - loss: 0.6176 - acc: 0.6793
800/800 [==============================] - 0s - loss: 0.6213 - acc: 0.6788     
Epoch 10/10

 32/800 [>.............................] - ETA: 0s - loss: 0.7265 - acc: 0.5938
256/800 [========>.....................] - ETA: 0s - loss: 0.6276 - acc: 0.6758
576/800 [====================>.........] - ETA: 0s - loss: 0.5997 - acc: 0.6944
800/800 [==============================] - 0s - loss: 0.6151 - acc: 0.6850     
[0.2554033  0.416494   0.82233113 0.5280267  0.22758274 0.20059742
 0.09272466 0.3564903  0.20322508 0.57580286 0.2500722  0.31531698
 0.20513508 0.4467136  0.20600389 0.17232506 0.29382464 0.45051524
 0.11957537 0.44465363 0.67702615 0.25206095 0.09765337 0.3506394
 0.4700673  0.20688586 0.5884992  0.5628815  0.21766251 0.57515
 0.4672349  0.4844754  0.21370926 0.28425097 0.35731304 0.6941308
 0.32472548 0.20581572 0.5935317  0.447108   0.3231491  0.40414882
 0.47661406 0.18218228 0.37868527 0.12896171 0.43613863 0.18641374
 0.46654457 0.75956804 0.42264277 0.03226176 0.48086402 0.6067783
 0.27309993 0.40649232 0.924935   0.23365851 0.44842258 0.21370926
 0.15399441 0.3443147  0.25733146 0.45903644 0.35186112 0.18185924
 0.33928016 0.56853807 0.2279914  0.45575368 0.25021252 0.469228
 0.18066035 0.10959466 0.45141518 0.41957211 0.36209774 0.3356553
 0.203554   0.60903585 0.46744087 0.1865246  0.35936862 0.2809527
 0.24704497 0.47803903 0.3259704  0.54414326 0.4085676  0.47358268
 0.20177633]
 #Excellent work! You're now ready to begin learning how to fine-tune your models.