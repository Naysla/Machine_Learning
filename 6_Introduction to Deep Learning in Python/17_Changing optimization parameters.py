#Changing optimization parameters
#It's time to get your hands dirty with optimization. You'll now try optimizing a model at a very low learning rate, a very high learning rate, and a "just right" learning rate. You'll want to look at the results after running this exercise, remembering that a low value for the loss function is good.
#
#For these exercises, we've pre-loaded the predictors and target values from your previous classification models (predicting who would survive on the Titanic). You'll want the optimization to start from scratch every time you change the learning rate, to give a fair comparison of how each learning rate did in your results. So we have created a function get_new_model() that creates an unoptimized model to optimize.


# Import the SGD optimizer
from keras.optimizers import SGD

# Create list of learning rates: lr_to_test
lr_to_test = [0.000001,0.01,1]

# Loop over learning rates
for lr in lr_to_test:
    print('\n\nTesting model with learning rate: %f\n'%lr )
    
    # Build new model to test, unaffected by previous models
    model = get_new_model()
    
    # Create SGD optimizer with specified learning rate: my_optimizer
    my_optimizer = SGD(lr=lr)
    
    # Compile the model
    model.compile(optimizer=my_optimizer,loss='categorical_crossentropy')
    
    # Fit the model
    model.fit(predictors, target)
	
#Answer
Testing model with learning rate: 0.000001

Epoch 1/10

 32/891 [>.............................] - ETA: 1s - loss: 3.6053
192/891 [=====>........................] - ETA: 0s - loss: 3.6579
288/891 [========>.....................] - ETA: 0s - loss: 3.8854
640/891 [====================>.........] - ETA: 0s - loss: 3.6788
891/891 [==============================] - 0s - loss: 3.6057     
Epoch 2/10

 32/891 [>.............................] - ETA: 0s - loss: 3.5751
288/891 [========>.....................] - ETA: 0s - loss: 3.8650
512/891 [================>.............] - ETA: 0s - loss: 3.5372
640/891 [====================>.........] - ETA: 0s - loss: 3.5092
864/891 [============================>.] - ETA: 0s - loss: 3.5422
891/891 [==============================] - 0s - loss: 3.5656     
Epoch 3/10

 32/891 [>.............................] - ETA: 0s - loss: 2.6692
384/891 [===========>..................] - ETA: 0s - loss: 3.3624
768/891 [========================>.....] - ETA: 0s - loss: 3.5500
891/891 [==============================] - 0s - loss: 3.5255     
Epoch 4/10

 32/891 [>.............................] - ETA: 0s - loss: 3.0058
416/891 [=============>................] - ETA: 0s - loss: 3.4565
608/891 [===================>..........] - ETA: 0s - loss: 3.4634
864/891 [============================>.] - ETA: 0s - loss: 3.4265
891/891 [==============================] - 0s - loss: 3.4854     
Epoch 5/10

 32/891 [>.............................] - ETA: 0s - loss: 2.5452
192/891 [=====>........................] - ETA: 0s - loss: 3.2657
256/891 [=======>......................] - ETA: 0s - loss: 3.2091
576/891 [==================>...........] - ETA: 0s - loss: 3.4479
891/891 [==============================] - 0s - loss: 3.4454     
Epoch 6/10

 32/891 [>.............................] - ETA: 0s - loss: 3.4446
384/891 [===========>..................] - ETA: 0s - loss: 3.5041
608/891 [===================>..........] - ETA: 0s - loss: 3.4844
891/891 [==============================] - 0s - loss: 3.4056     
Epoch 7/10

 32/891 [>.............................] - ETA: 0s - loss: 4.1073
352/891 [==========>...................] - ETA: 0s - loss: 3.5075
768/891 [========================>.....] - ETA: 0s - loss: 3.3854
891/891 [==============================] - 0s - loss: 3.3659     
Epoch 8/10

 32/891 [>.............................] - ETA: 0s - loss: 3.0972
320/891 [=========>....................] - ETA: 0s - loss: 3.1479
544/891 [=================>............] - ETA: 0s - loss: 3.2467
800/891 [=========================>....] - ETA: 0s - loss: 3.3275
891/891 [==============================] - 0s - loss: 3.3263     
Epoch 9/10

 32/891 [>.............................] - ETA: 0s - loss: 3.7464
320/891 [=========>....................] - ETA: 0s - loss: 3.1807
576/891 [==================>...........] - ETA: 0s - loss: 3.2847
891/891 [==============================] - 0s - loss: 3.2867     
Epoch 10/10

 32/891 [>.............................] - ETA: 0s - loss: 3.3862
256/891 [=======>......................] - ETA: 0s - loss: 3.2630
384/891 [===========>..................] - ETA: 0s - loss: 3.2149
736/891 [=======================>......] - ETA: 0s - loss: 3.1780
891/891 [==============================] - 0s - loss: 3.2473     


Testing model with learning rate: 0.010000

Epoch 1/10

 32/891 [>.............................] - ETA: 1s - loss: 1.0910
320/891 [=========>....................] - ETA: 0s - loss: 2.4560
736/891 [=======================>......] - ETA: 0s - loss: 1.7049
891/891 [==============================] - 0s - loss: 1.5757     
Epoch 2/10

 32/891 [>.............................] - ETA: 0s - loss: 1.4557
384/891 [===========>..................] - ETA: 0s - loss: 1.2603
576/891 [==================>...........] - ETA: 0s - loss: 1.2848
891/891 [==============================] - 0s - loss: 1.4504     
Epoch 3/10

 32/891 [>.............................] - ETA: 0s - loss: 0.7488
352/891 [==========>...................] - ETA: 0s - loss: 1.4439
448/891 [==============>...............] - ETA: 0s - loss: 1.3975
672/891 [=====================>........] - ETA: 0s - loss: 1.2500
864/891 [============================>.] - ETA: 0s - loss: 1.4601
891/891 [==============================] - 0s - loss: 1.4771     
Epoch 4/10

 32/891 [>.............................] - ETA: 0s - loss: 1.1953
384/891 [===========>..................] - ETA: 0s - loss: 2.1050
608/891 [===================>..........] - ETA: 0s - loss: 1.8558
800/891 [=========================>....] - ETA: 0s - loss: 2.0221
891/891 [==============================] - 0s - loss: 1.9701     
Epoch 5/10

 32/891 [>.............................] - ETA: 0s - loss: 7.5367
288/891 [========>.....................] - ETA: 0s - loss: 3.2403
704/891 [======================>.......] - ETA: 0s - loss: 2.2596
891/891 [==============================] - 0s - loss: 2.1955     
Epoch 6/10

 32/891 [>.............................] - ETA: 0s - loss: 1.2908
448/891 [==============>...............] - ETA: 0s - loss: 1.5194
768/891 [========================>.....] - ETA: 0s - loss: 1.4904
891/891 [==============================] - 0s - loss: 1.4723     
Epoch 7/10

 32/891 [>.............................] - ETA: 0s - loss: 1.2749
224/891 [======>.......................] - ETA: 0s - loss: 2.6698
608/891 [===================>..........] - ETA: 0s - loss: 2.3830
891/891 [==============================] - 0s - loss: 2.3448     
Epoch 8/10

 32/891 [>.............................] - ETA: 0s - loss: 0.7451
320/891 [=========>....................] - ETA: 0s - loss: 2.2301
704/891 [======================>.......] - ETA: 0s - loss: 2.0936
891/891 [==============================] - 0s - loss: 2.3312     
Epoch 9/10

 32/891 [>.............................] - ETA: 0s - loss: 2.7462
192/891 [=====>........................] - ETA: 0s - loss: 5.2042
544/891 [=================>............] - ETA: 0s - loss: 8.3848
891/891 [==============================] - 0s - loss: 8.9001     
Epoch 10/10

 32/891 [>.............................] - ETA: 0s - loss: 10.0738
416/891 [=============>................] - ETA: 0s - loss: 10.6550
672/891 [=====================>........] - ETA: 0s - loss: 10.3856
891/891 [==============================] - 0s - loss: 9.9301      


Testing model with learning rate: 1.000000

Epoch 1/10

 32/891 [>.............................] - ETA: 2s - loss: 1.0273
320/891 [=========>....................] - ETA: 0s - loss: 5.6937
672/891 [=====================>........] - ETA: 0s - loss: 5.6615
891/891 [==============================] - 0s - loss: 5.9885     
Epoch 2/10

 32/891 [>.............................] - ETA: 0s - loss: 4.5332
224/891 [======>.......................] - ETA: 0s - loss: 6.1882
448/891 [==============>...............] - ETA: 0s - loss: 6.0083
576/891 [==================>...........] - ETA: 0s - loss: 6.0723
704/891 [======================>.......] - ETA: 0s - loss: 6.3419
891/891 [==============================] - 0s - loss: 6.1867     
Epoch 3/10

 32/891 [>.............................] - ETA: 0s - loss: 7.0517
448/891 [==============>...............] - ETA: 0s - loss: 5.8284
864/891 [============================>.] - ETA: 0s - loss: 6.1935
891/891 [==============================] - 0s - loss: 6.1867     
Epoch 4/10

 32/891 [>.............................] - ETA: 0s - loss: 6.0443
 96/891 [==>...........................] - ETA: 0s - loss: 6.5480
384/891 [===========>..................] - ETA: 0s - loss: 6.2542
768/891 [========================>.....] - ETA: 0s - loss: 6.1492
891/891 [==============================] - 0s - loss: 6.1867     
Epoch 5/10

 32/891 [>.............................] - ETA: 0s - loss: 9.0664
448/891 [==============>...............] - ETA: 0s - loss: 6.0803
736/891 [=======================>......] - ETA: 0s - loss: 6.0881
891/891 [==============================] - 0s - loss: 6.1867     
Epoch 6/10

 32/891 [>.............................] - ETA: 0s - loss: 6.0443
416/891 [=============>................] - ETA: 0s - loss: 6.2380
672/891 [=====================>........] - ETA: 0s - loss: 6.2122
891/891 [==============================] - 0s - loss: 6.1867     
Epoch 7/10

 32/891 [>.............................] - ETA: 0s - loss: 5.0369
416/891 [=============>................] - ETA: 0s - loss: 6.5480
608/891 [===================>..........] - ETA: 0s - loss: 6.4684
891/891 [==============================] - 0s - loss: 6.1867     
Epoch 8/10

 32/891 [>.............................] - ETA: 0s - loss: 5.0369
128/891 [===>..........................] - ETA: 0s - loss: 6.0443
512/891 [================>.............] - ETA: 0s - loss: 5.9498
736/891 [=======================>......] - ETA: 0s - loss: 6.0662
891/891 [==============================] - 0s - loss: 6.1867     
Epoch 9/10

 32/891 [>.............................] - ETA: 0s - loss: 5.5406
288/891 [========>.....................] - ETA: 0s - loss: 6.1003
704/891 [======================>.......] - ETA: 0s - loss: 6.0672
891/891 [==============================] - 0s - loss: 6.1867     
Epoch 10/10

 32/891 [>.............................] - ETA: 0s - loss: 5.5406
416/891 [=============>................] - ETA: 0s - loss: 6.6642
800/891 [=========================>....] - ETA: 0s - loss: 6.1450
891/891 [==============================] - 0s - loss: 6.1867
